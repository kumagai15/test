# TUAT Open Music Dataset for Research

## Overview
We created forty-five pieces that consisted of melodies produced by piano sounds without harmony.
The musical stimuli (MIDI) were synthesized by Sibelius (Avid Technology, USA), a music computation and notation program. 
The sound intensities of all of the generated musical pieces were identical.
The length of each musical piece was 34 s, with the tempo set to 150 beats per minute (bpm) (i.e., the frequency of a quarter of a note was 2.5 Hz). 
We set the sampling frequency to 44,100 Hz.

## Dataset
Music for the audio stimuli are shown in [music_list.csv](music_list.csv).<br>
Both [MIDI](/midi) and [WAVE](/wave) files are availavle.


## Publication
A full data descriptor is published in [Frontiers](https://www.frontiersin.org/articles/10.3389/fnhum.2018.00444/full). If you use TUAT Open Music Dataset in your research, please cite the following paper.

```bibtex
@article{kumagai2018music,
    title={Music familiarity affects {EEG} entrainment when little attention is paid},
    author={Kumagai, Yuiko and Matsui, Ryosuke and Tanaka, Toshihisa},
    journal={Frontiers in Human Neuroscience},
    volume={12},
    pages={444},
    year={2018},
    publisher={Frontiers}
}
```

## Licenses
- The metadata is released under the Creative Commons Attribution 4.0 International License (CC BY 4.0).
- We do not hold the copyright on the audio and distribute it under the license chosen by the artist.
- The dataset is meant for research purposes.

## Contact
For any queries, please contact:

> Toshihisa Tanaka ([TUAT Biosignal Informatics Lab](https://www.sip.tuat.ac.jp/))<br>
> Email: tanakat (at) cc.tuat.ac.jp
